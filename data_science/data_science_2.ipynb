{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/unfamiliarplace/acse-integration/blob/main/data_science/data_science_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Funfamiliarplace%2Facse-integration&branch=main&subPath=data_science%2Fdata_science_2.ipynb&depth=2\"  target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Part 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three data science notebooks\n",
    "\n",
    "Here's what we've covered and will cover in this series:\n",
    "\n",
    "Part 1: We examined Python's core data structures and saw some simple visualizations.\n",
    "\n",
    "**Part 2: We will explore and learn to use Python's dedicated data science tools in more depth.**\n",
    "\n",
    "Part 3: We will apply our knowledge to a project with multiple steps and visualizations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of this notebook\n",
    "\n",
    "The goal of this notebook is to learn how to use Python's dedicated data science tools, in a way that you can take and reuse with students.\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "**1. Representing data:** Choosing the right structures and labels for a dataset.\n",
    "\n",
    "**2. Analyzing data:** Transforming, selecting, and calculating statistics on data.\n",
    "\n",
    "**Conclusion**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Data science** may seem like a complex topic, but it boils down to this:\n",
    "\n",
    "> There's a ton of data out there! How do I find the information I'm looking for? How do I understand it? And how do I make it relevant?\n",
    "\n",
    "In other words, data science is the science of handling data in order to *answer a specific question*.\n",
    "\n",
    "Python allows us to do this by providing us with tools for representing data, manipulating or selecting from it, analyzing it, and creating charts, graphs, maps, and other visualizations that display it. In this notebook, we are learning how to use these tools. (In the next notebook, we will take a closer look at gathering and cleaning data from various real-world sources so that we can put our skills together for a specific purpose.)\n",
    "\n",
    "### Learning goals\n",
    "\n",
    "* A1. demonstrate the ability to use different data types, including one-dimensional arrays, in computer programs.\n",
    "\n",
    "* A2. demonstrate the ability to use control structures and simple algorithms in computer programs.\n",
    "\n",
    "### Success criteria\n",
    "\n",
    "* I can choose and implement a structure to represent a dataset in code.\n",
    "\n",
    "* I can manipulate a data structure in code to select, organize, and analyze data.\n",
    "\n",
    "> [Source: Ontario Curriculum (2008)](https://www.edu.gov.on.ca/eng/curriculum/secondary/computer10to12_2008.pdf#page=41)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Representing data\n",
    "\n",
    "As we saw in the last notebook, often the greatest power in Python comes from the use of external libraries. Two core libraries for data science are `numpy` (Numerical Python) and `pandas` (Python for Data Analysis).\n",
    "\n",
    "We will focus on `pandas` for this lesson to keep things simple. (However, note that `pandas` is actually built on `numpy`, and uses `numpy` datatypes like an `ndarray` instead of Python's built-in `list`.)\n",
    "\n",
    "Run this code block to ensure you have `pandas` installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install numpy and pandas to the current environment\n",
    "# N.B. pandas should install numpy automatically since it's a prerequisite\n",
    "\n",
    "%pip install pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run this block to import it. Remember that in Jupyter Notebooks, all cells share a memory pool, so if we import it at the start then they'll be available for all future blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "There are two core data structures in `pandas`: `Series` and `DataFrame`.\n",
    "\n",
    "A `Series` is essentially a one-dimensional array, like a `list`. Run this code block to see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Series representing some mysterious data\n",
    "s = pd.Series([200, 210, 215, 230, 291])\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What makes a Series unique?\n",
    "\n",
    "That output is a little surprising for a one-dimensional array! We notice that it's presented like a table, with one column for the index and another for the data.\n",
    "\n",
    "That gives us a hint about the first unique quality of a `Series`: You can use any kind of index, not just a numerical count from `0`.\n",
    "\n",
    "Let's try making a custom index. Run this code block. P.S. Can you guess what this data represents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using strings for the index\n",
    "s = pd.Series([200, 210, 215, 230, 291],\n",
    "              index=['Skor', 'Heath', 'Snickers', 'Mars', 'Twix'])\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the \"rows\" of our table each have a name. You might be thinking now that this is kind of like a dictionary, with key-value pairs, and that's a valid observation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Check:** Notice that we have the same number of values and labels. What if those two lists were different lengths? Make a guess and then try it out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there's a weird thing there at the end of our `Series` output: a `dtype`! This datatype is given because a `Series` can only contain *one* type of data for *all* its elements. This is one of the key principles of having clean datasets: a restriction like this helps us avoid comparing apples and oranges, or rather 5s and oranges.\n",
    "\n",
    "In this case, `pandas` guessed the datatype we wanted, a 64-bit integer. This is a very generous guess because an `int64`'s max value is `2^64` or about 9 quintillion. We can optimize our `Series` by specifiying the datatype if we know the bounds of our dataset. Let's use `int16`, which gives us an overheard of `2^16` or `65,536`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying a datatype\n",
    "s = pd.Series([200, 210, 215, 230, 291],\n",
    "              index=['Skor', 'Heath', 'Snickers', 'Mars', 'Twix'],\n",
    "              dtype='int16')\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Check:** What would happen if you used an even slimmer datatype of `int8`? Make a guess and then try it out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can I do with a Series?\n",
    "\n",
    "Here's a [full reference](https://pandas.pydata.org/docs/reference/api/pandas.Series.html), but let's take a look at some common tasks.\n",
    "\n",
    "You can run each of these code blocks to see how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the values\n",
    "s.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific item\n",
    "s.loc['Skor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of the Series\n",
    "s.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the highest value\n",
    "s.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lowest value\n",
    "s.min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one will show you something a Python list can't easily do. We found the maximum and minimum values; what if we want to know which chocolate bars those were? Simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the chocolate bar with the highest value\n",
    "s.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the chocolate bar with the lowest value\n",
    "s.idxmin()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort a `Series` [by its values](https://pandas.pydata.org/docs/reference/api/pandas.Series.sort_values.html) or [by its indices](https://pandas.pydata.org/docs/reference/api/pandas.Series.sort_index.html). (Note that unlike a list, this is *not* done in place unless you specify the keyword argument `inplace=True`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting a Series by value\n",
    "print(s.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting a Series by index\n",
    "print(s.sort_index())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some very cool operations on the whole `Series` at once. Can your grandma's `list` do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to all items\n",
    "s = s + 100\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply all items\n",
    "s = s * 5\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can carry out any function on all elements of a `Series`. Let's get the square root of each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map a function\n",
    "from math import sqrt\n",
    "s = s.map(sqrt)\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, those decimals aren't very pretty. Let's round them to 2 decimal places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round a Series\n",
    "s = s.round(2)\n",
    "print(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Check:** Try a few more operations on this `Series`: subtraction, division, exponentiation, negation.\n",
    "\n",
    "Feel free to run this block to reset your `Series` if you need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([200, 210, 215, 230, 291],\n",
    "              index=['Skor', 'Heath', 'Snickers', 'Mars', 'Twix'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much more we can do with a `Series`, including:\n",
    "\n",
    "* Filtering data\n",
    "* Correlating data\n",
    "* Analyzing data (e.g. averages)\n",
    "\n",
    "We'll save these for the next section, though."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "This is the other major structure in `pandas`. Once we understand a `Series`, a `DataFrame` is not too complicated: it's just a bunch of `Series` glued together into a table. In other words, it's two-dimensional.\n",
    "\n",
    "Again, [here's the full reference](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), but we'll just look at a few aspects of it.\n",
    "\n",
    "Let's start by simply throwing our existing `Series` into a `DataFrame`. When we do so, we'll give it a name, so I'll now reveal what those values represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from a Series\n",
    "s = pd.Series([200, 210, 215, 230, 291],\n",
    "              index=['Skor', 'Heath', 'Snickers', 'Mars', 'Twix'],\n",
    "              name='Calories per package')\n",
    "df = pd.DataFrame(s)\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far it looks the same. But we can now add another `Series` to it and grow our table.\n",
    "\n",
    "Note that we don't actually need to name our new `Series` because the `DataFrame` needs a column name anyway. We do need to make sure the indices match, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another Series to a DataFrame\n",
    "s2 = pd.Series([39, 39, 44, 51, 58],\n",
    "               index=['Skor', 'Heath', 'Snickers', 'Mars', 'Twix'])\n",
    "df['Grams per package'] = s2\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There! Now it looks more like a table.\n",
    "\n",
    "**Understanding Check:** Can you add a column that lists the deliciousness level, on a scale from 1 to 10, for each bar? (You can make up the values, but Mars is definitely more delicious than Snickers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a third column for 'Deliciousness'\n",
    "# TODO ADD YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other ways of making DataFrames\n",
    "\n",
    "We made a `DataFrame` this way because we looked at `Series` first, but we could also have created one from scratch like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a DataFrame without first making a Series\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Calories per package': [200, 210, 215, 230, 291],\n",
    "    'Grams per package': [39, 39, 44, 51, 58],\n",
    "    'Deliciousness': [5, 4, 8, 6, 2]\n",
    "}, index=['Skor', 'Heath', 'Snickers', 'Mars', 'Twix'])\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, an alternative — and equally good — way to represent this data would be to take the name of the chocolate bar as an additional column.\n",
    "\n",
    "If we do so, the indices could be `0, 1, 2, 3, 4`, or we could even use the chocolate bar name in both places. It's a matter of preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default 0-based numerical index, data as column\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Chocolate bar': ['Skor', 'Heath', 'Snickers', 'Mars', 'Twix'],\n",
    "    'Calories per package': [200, 210, 215, 230, 291],\n",
    "    'Grams per package': [39, 39, 44, 51, 58],\n",
    "    'Deliciousness': [5, 4, 8, 6, 2]\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same data as both index and column\n",
    "\n",
    "bars = ['Skor', 'Heath', 'Snickers', 'Mars', 'Twix']\n",
    "df = pd.DataFrame({\n",
    "    'Chocolate bar': bars,\n",
    "    'Calories per package': [200, 210, 215, 230, 291],\n",
    "    'Grams per package': [39, 39, 44, 51, 58],\n",
    "    'Deliciousness': [5, 4, 8, 6, 2]\n",
    "}, index=bars)\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing data\n",
    "\n",
    "All right, let's try to juggle this thing a bit and see what we can make come out of it.\n",
    "\n",
    "First, in order to do more interesting things, we'll want a bigger dataset. We'll use `pandas`' `read_csv` function to open a comma-separated values file from GitHub that contains some data on the weather as measured in January 2023 at Pearson Airport in Toronto ([source](https://climate.weather.gc.ca/climate_data/daily_data_e.html?StationID=51459));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from a remote CSV\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/unfamiliarplace/acse-integration/main/data_science/data/2023-01_51459.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S. You can also use your own files for data. If you're using notebooks in an IDE like VSCode, you can just use local files. If you're using Google Colab or Calysto, you can upload files to the instance. Then you would read them using standard I/O functions like this:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from a local CSV\n",
    "import pandas as pd\n",
    "path = \"data/2023-01_51459.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go ahead and just `print(df)`, we should realize that this could be a long file. If we want to see what sort of thing it contains, we don't need to look at all of it — the first couple of rows should be enough.\n",
    "\n",
    "To do that, we use `DataFrame.head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the head or first few rows of a DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this DataFrame is numerically indexed, and it has 5 columns: a date, a maximum and minimum temperature in degrees Celsius, rain in millimetres, and snow in centimetres.\n",
    "\n",
    "First of all, let's set the date as the index. It's not really weather data; it's the label that each row of data should be known by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing date from column to index\n",
    "df = df.set_index('Date', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to navigate to some particular data. You might remember `loc` from when we used `Series` above. We can use that here too. I remember that I moved to a new apartment on January 8, but I forget if it was raining that day. Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting one row of the frame\n",
    "print(df.loc['2023-01-08'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was not. But it was pretty cold!\n",
    "\n",
    "**Understanding Check:** Take a closer look at that output. What type do you think it has?\n",
    "\n",
    "<details><summary>Click to reveal</summary>\n",
    "\n",
    "It's a `Series`! It has a `dtype` and a `name`, which is the column header. This might seem strange if you remember that earlier, a `Series` was a column of a `DataFrame`. In other words, the data must have been transposed here. Sure enough, the `DataFrame`'s column headers become the row indices!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe I only care about the minimum temperature and not all that other stuff. Can I get that? Yup — via a pair of indices in a `loc`, much like the coordinates of a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting one cell of the frame\n",
    "print(df.loc['2023-01-08', 'Min Temp (C)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I get just one *column*? Yes, by directly indexing instead of using `loc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting one column of the frame\n",
    "print(df['Min Temp (C)'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this, too, comes back as a `Series` with the row indices intact."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, we can sort `DataFrames` just as we can `Series` [by values](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) or [by indices](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_index.html), with the additional step that we need to specify which column to sort by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting DataFrame by column\n",
    "print(df.sort_values(by='Min Temp (C)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting DataFrame by index\n",
    "print(df.sort_index())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying\n",
    "\n",
    "Let's add a column of our own that might be handy to work with. We have a maximum and a minimum temperature; let's add a range column.\n",
    "\n",
    "Here's where some of that `Series` magic we saw earlier comes into play! No looping is needed; we can just do an operation on a whole column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a new column based on two old ones\n",
    "range_series = df['Max Temp (C)'] - df['Min Temp (C)']\n",
    "df['Temp Range (C)'] = range_series\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I might want to place that new column after the other temperature ones. I can reassign the columns like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign column order\n",
    "df = df[['Max Temp (C)', 'Min Temp (C)', 'Temp Range (C)', 'Rain (mm)', 'Snow (cm)']]\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, how about the fact that snow is in cm but rain is in mm? Let's convert one to the other. I think we'll change snow to mm so that we don't have to worry about losing precision.\n",
    "\n",
    "Hmmm... first, though, I notice that it didn't snow at all in the first few days of January. All the values are `0.0`. That means we won't be able to check our calculation. But we can also use `tail` instead of `head` to preview the last few days, which did have snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating a column's data\n",
    "df['Snow (cm)'] = df['Snow (cm)'] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice — it worked. We'd better update the column label too. How do we do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a column\n",
    "df = df.rename(columns={'Snow (cm)': 'Snow (mm)'})\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Check:** Can you add a final column for total precipitation? You won't need to reorder the columns since it'll already come after rain and snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for total precipitation\n",
    "# TODO ADD YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```\n",
    "df['Total precipitation (mm)'] = df['Rain (mm)'] + df['Snow (mm)']\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing\n",
    "\n",
    "Let's start to look around this data a little. While you could print the whole `DataFrame` and inspect it by hand, it might be hard to get a sense of the trends by doing that.\n",
    "\n",
    "At this stage, we are trying to do an important thing: **identify interesting things we might want to investigate further**. In other words, we want to know some questions we can ask of this data.\n",
    "\n",
    "We can begin with some stabs in the dark based on intuitions such as:\n",
    "\n",
    "* It snows a lot in January.\n",
    "\n",
    "* January is very cold."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does it snow more than it rains?\n",
    "\n",
    "This one should be pretty straightforward; let's total the rainfall and snowfall amounts and see which is greater.\n",
    "\n",
    "To do that, we can sum each `Series` and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing Series\n",
    "total_rain = df['Rain (mm)'].sum()\n",
    "total_snow = df['Snow (mm)'].sum()\n",
    "\n",
    "print(f'Total rain: {total_rain:>5} mm')\n",
    "print(f'Total snow: {total_snow:>5} mm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, there's almost 9 times as much snow as rain! I wonder how many more *days* are snowy than rainy?\n",
    "\n",
    "To find that out, we can filter the `DataFrame` by whether the value in the relevant column is above `0`. This introduces us to a powerful feature of `Series`: applying a boolean operation to each element in one pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering a DataFrame\n",
    "rainy_days = df[df['Rain (mm)'] > 0]\n",
    "snowy_days = df[df['Snow (mm)'] > 0]\n",
    "\n",
    "# In DataFrames, size refers to both dimensions.\n",
    "# To get just the number of rows, get the \"shape\" of the first column.\n",
    "n_rainy_days = rainy_days.shape[0]\n",
    "n_snowy_days = snowy_days.shape[0]\n",
    "\n",
    "print(f'Rainy days: {n_rainy_days:>2}')\n",
    "print(f'Snowy days: {n_snowy_days:>2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's surprising! Even though there was much more snow than rain, there were almost as many rainy days as snowy ones. That raises interesting questions that we could pursue for a deeper understanding.\n",
    "\n",
    "Before we ask a different question, though, let's check one more factor. I noticed when we looked at the `tail` of the `DataFrame` that some days had both snow *and* rain. How many of those were there?\n",
    "\n",
    "We can filter like before,  and we can use the usual logic to combine conditions (both snow and rain are above `0`). *However*, in `pandas`, we will use slightly different operators when we are trying to combine multiple conditions on individual elements:\n",
    "\n",
    "| Operation | Normal Python | pandas |\n",
    "| --- | --- | --- |\n",
    "| both must be true | `and` | `&` |\n",
    "| at least one must be true | `or` | `|` |\n",
    "| must be false | `not` | `~` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for days when it rained and snowed\n",
    "both_days = df[(df['Rain (mm)'] > 0) & (df['Snow (mm)'] > 0)]\n",
    "n_both_days = df.shape[0]\n",
    "\n",
    "print(f'Days when it both snowed and rained: {n_rainy_days:>2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... can you make anything of that fact? Or do you need more info?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How cold is January?\n",
    "\n",
    "A simple way to start might be to try averaging the high and low temperatures across the whole month. We can do this by isolating the respective `Series` and using `mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging the high\n",
    "print(df['Min Temp (C)'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging the high\n",
    "print(df['Max Temp (C)'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more fulsome picture, let's also get the median, range, and mode. We'll define a function to simply doing this for both low and high.\n",
    "\n",
    "The mode is a little more complex since (1) we have to round first, and (2) there can be more than one mode, so we should anticipate a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for showing all four averages\n",
    "def show_averages(s: pd.Series, word: str) -> None:\n",
    "    s_mean = s.mean()\n",
    "    s_median = s.median()\n",
    "    s_range = s.max() - s.min()\n",
    "    s_modes = s.round(0).mode().values\n",
    "\n",
    "    print(f'{word} mean:\\t{s_mean}')\n",
    "    print(f'{word} median:\\t{s_median}')\n",
    "    print(f'{word} range:\\t{s_range}')\n",
    "    print(f'{word} mode(s):\\t{s_modes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complete averages of the low temperatures\n",
    "show_averages(df['Min Temp (C)'], 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complete averages of the high temperatures\n",
    "show_averages(df['Max Temp (C)'], 'High')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that each average tells a slightly different story. One thing we might notice is that for the lows, the mean and median are much closer to the mode, suggesting coherence: a random day in January probably went down to about -3 degrees Celsius. But the high had a lot of days that were 2-3 degrees warmer than the average.\n",
    "\n",
    "Also, we notice that the ranges are quite large for both, but especially the low! That suggests that there was at least one very warm or very cold day for January. Can we remove the outliers so that our picture of the average is more accurate?\n",
    "\n",
    "We'll look at two ways in which we could remove outliers. But first, in order to make this data more interesting, we're going to switch over to a dataset with the full 2022 data to ensure our sample size is sufficient.\n",
    "\n",
    "Run this next code block to catch up a big dataset on all we've done so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load larger dataset and catch up the changes\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/unfamiliarplace/acse-integration/main/data_science/data/2022_51459.csv\"\n",
    "df_big = pd.read_csv(url)\n",
    "df_big = df_big[['Max Temp (C)', 'Min Temp (C)', 'Rain (mm)', 'Snow (cm)']]\n",
    "df_big['Snow (cm)'] = df_big['Snow (cm)'] * 10\n",
    "df_big = df_big.rename(columns={'Snow (cm)': 'Snow (mm)'})\n",
    "df_big['Total precipitation (mm)'] = df_big['Rain (mm)'] + df_big['Snow (mm)']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we could take advantage of the `Series.quantile` method, which takes a decimal representing a percentage and returns the cutoff value for that quantile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removing top and bottom 10 quantiles\n",
    "def remove_outliers_quantile(s: pd.Series) -> pd.Series:\n",
    "    # Determine the value at the 90th and 10th quantiles\n",
    "    q_upper = s.quantile(0.9)\n",
    "    q_lower = s.quantile(0.1)\n",
    "\n",
    "    # Create another Series by filtering for elements that are between the upper and lower bounds\n",
    "    return s[(s < q_upper) & (s > q_lower)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we could take advantage of the `Series.std` method, which returns the standard deviation (average distance between a given data point and the mean). We could then cut off datapoints further than `n` standard deviations away from the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removing values outisde n standard deviations\n",
    "def remove_outliers_std(s: pd.Series, max_deviations: float) -> pd.Series:\n",
    "    # Determine the cutoffs as a multiple of standard deviations    \n",
    "    max_distance = s.std() * max_deviations\n",
    "\n",
    "    # Create another Series by filtering for elements that are between the upper and lower bounds\n",
    "    mean = s.mean()\n",
    "    return s[(s < (mean + max_distance)) & (s > (mean - max_distance))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the standard deviation method and see how that changes the averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low seriesa averages without outliers\n",
    "low_without_outliers = remove_outliers_std(df_big['Min Temp (C)'], 1)\n",
    "show_averages(low_without_outliers, 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High seriesa averages without outliers\n",
    "low_without_outliers = remove_outliers_std(df_big['Max Temp (C)'], 1)\n",
    "show_averages(low_without_outliers, 'High')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the low mean is not as cold, the low modes no longer include positive 2 degrees, and the high mode has sunk by 3 degrees and is now much closer to its mean and median. That suggests that January had a significant number of warm-ish days where the temperature hovered between 2 and 4 degrees or so. Of course, this is not yet a conclusion; it's just a direction to investigate more closely."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations\n",
    "\n",
    "What are some other intuitive ideas you might have about January?\n",
    "\n",
    "Maybe you believe it generally snows if the temperature stays at 0 or below, but rains if the temperature stays above 0. Let's just see!\n",
    "\n",
    "One way to do this is by checking **correlation**, which is a feature of `Series`. A correlation ranges from `-1` (the two events happen at opposite times) to `1` (the two events always co-occur).\n",
    "\n",
    "First, we'll binarize one of the columns: we'll take our snowfall amount and turn it into `True/False` to represent whether it snowed. That gives us a continuous variable (daily high temperature) and a binary variable (snowfall). That means we're essentially doing a [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), where the temperature will be treated as a predictor of whether it snowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns\n",
    "df_big['Did it snow?'] = df_big['Snow (mm)'] > 0\n",
    "print(df_big.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation\n",
    "corr = df_big['Did it snow?'].corr(df_big['Min Temp (C)'])\n",
    "print(corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a decent negative correlation. That means it tends to snow when the temperature is lower, which is what we'd expect.\n",
    "\n",
    "However, this correlation could actually be stronger. Consider that there must be many days when it neither snows nor rains, and we don't want to count those: we know that being cold doesn't itself cause precipitation! So let's start again, but first we'll filter for days that had any precipitation. We can filter our `DataFrame` using the `Total precipitation (mm)` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating more logical subset\n",
    "df_big_with_precip = df_big[df_big['Total precipitation (mm)'] > 0].copy()\n",
    "df_big_with_precip['Did it snow?'] = df_big_with_precip['Snow (mm)'] > 0\n",
    "\n",
    "print(df_big_with_precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redoing the correlation\n",
    "corr = df_big_with_precip['Did it snow?'].corr(df_big_with_precip['Min Temp (C)'])\n",
    "print(corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a negative correlation that's another 50% greater. This is a good sign and matches our intuitions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Check:** Can you test the inverse correlation: whether rain can be predicted by a high temperature? (You should find a positive correlation between temperature and whether it rained.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the \"rainy on warm days\" correlation\n",
    "# TODO ADD YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built up a lot of tools throughout this notebook. Specifically, we've learned:\n",
    "\n",
    "* What a `Series` is: linear data that can have labels\n",
    "* What a `DataFrame` is: a table where each column is a `Series`\n",
    "* How to view, sort, filter, and modify these data structures\n",
    "* How to calculate basic statistics on data\n",
    "* How to manipulate the data into a form where we can start to use statistics to answer questions\n",
    "\n",
    "In the last notebook in this series, we'll put these skills together along with some data finding and cleaning techniques in order to answer a more complex real-world question.\n",
    "\n",
    "See you then!\n",
    "\n",
    "![Panda waving goodbye](https://raw.githubusercontent.com/unfamiliarplace/acse-integration/main/data_science/assets/panda_wave.jpg)\n",
    "<sub>*Panda image from [VCG Photo](https://news.cgtn.com/news/3d3d414e32557a4e30457a6333566d54/share_p.html)*</sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "458dd1d06817a72759ca62d766d5a1c58019d69edba750c2eb07d80bb7630974"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
