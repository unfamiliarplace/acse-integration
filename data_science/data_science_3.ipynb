{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/unfamiliarplace/acse-integration/blob/main/data_science/data_science_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Funfamiliarplace%2Facse-integration&branch=main&subPath=data_science%2Fdata_science_3.ipynb&depth=2\"  target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three data science notebooks\n",
    "\n",
    "Here's what we've covered and will cover in this series:\n",
    "\n",
    "Part 1: We examined Python's core data structures and saw some simple visualizations.\n",
    "\n",
    "Part 2: We will explore and learn to use Python's dedicated data science tools in more depth.\n",
    "\n",
    "**Part 3: We will apply our knowledge to a project with multiple steps and visualizations.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of this notebook\n",
    "\n",
    "The goal of this notebook is use Python's dedicated data science tools for a specific purpose related to data science.\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "**1. Finding and cleaning data:** Locating, downloading, and cleaning data for use.\n",
    "\n",
    "**2. Exploring, analyzing, and visualizing data:** Transforming, calculating statistics, and presenting the results of data.\n",
    "\n",
    "**Conclusion**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For both students and ourselves, an important question whenever we learn something is: \"Why are we learning this? What can I use it for?\" We've seen the basics of how to use the tools. As an English teacher I know likes to write in the margins of essays: \"So what?\"\n",
    "\n",
    "It's a challenging question, but the beautiful thing about this stage is that you can open it to students. If they've been prepared well, then the applications should be limitless. In data science, that means you should be able to ask a lot of different real-world questions and get interesting answers.\n",
    "\n",
    "However, it's not always possible to come up with an interesting question from thin air. Often, the best questions actually arise from seeing something curious or encountering some underexplained data or hearing an anecdote and wondering whether it's typical or exceptional.\n",
    "\n",
    "So let's go hunting!\n",
    "\n",
    "### Learning goals\n",
    "\n",
    "* A1. demonstrate the ability to use different data types, including one-dimensional arrays, in computer programs.\n",
    "\n",
    "* D2.2 demonstrate an understanding of an area of collaborative research between computer science and another field.\n",
    "\n",
    "### Success criteria\n",
    "\n",
    "* I can choose and implement a structure to represent a dataset in code.\n",
    "\n",
    "* I can manipulate a data structure in code to select, organize, and analyze data.\n",
    "\n",
    "* I can create a suitable visualization of a dataset in code in order to better understand a question.\n",
    "\n",
    "> [Source: Ontario Curriculum (2008)](https://www.edu.gov.on.ca/eng/curriculum/secondary/computer10to12_2008.pdf#page=41)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finding and cleaning data\n",
    "\n",
    "Where shall we begin looking for data? There are many paid and a few good free sources of significant datasets. Helping students comb through these is itself a great learning opportunity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data sources\n",
    "\n",
    "* [data.world](https://data.world): Large collection of datasets. Requires sign-up.\n",
    "\n",
    "* [LODE Databases](https://www.statcan.gc.ca/en/lode/databases): A Canadian gov't initiative to create open municipal, provincial, and federal datasets.\n",
    "\n",
    "* [Statistics Canada](https://www150.statcan.gc.ca/n1/en/type/data): Large collection of datasets on all things measurable in Canada (economy, geography, society, etc.)\n",
    "\n",
    "* [data.gov](https://data.census.gov/): The US Census Bureau's equivalent of StatsCan.\n",
    "\n",
    "* [World Bank](https://data.worldbank.org/): A global version of the same. Can also zoom into the country level.\n",
    "\n",
    "* [First Nations Information Governance Centre](https://fnigc.ca/): A Canadian First Nations initiative to create [Indigenous data sovereignty](https://guides.library.utoronto.ca/indigenousstudies/datasovereignty).\n",
    "\n",
    "* [World Health Organization](https://www.who.int/data/gho/): Global health-related data.\n",
    "\n",
    "* [Open Data Sciences Conference](https://odsc.com/): They don't directly link to data science sources, but they promote resources, e.g. [this Medium.com list of free natural language processing datasets](https://odsc.medium.com/20-open-datasets-for-natural-language-processing-538fbfaf8e38).\n",
    "\n",
    "* [PEW Research](https://www.webfx.com/blog/marketing/data-sources/): Survey-based data. Requires sign-up.\n",
    "\n",
    "* [AWS Datasets](https://registry.opendata.aws/): Useful, but a little harder to access because they require either AWS integration, the AWS CLI, or following the links the original authors' repositories.\n",
    "\n",
    "* [Many more](https://www.webfx.com/blog/marketing/data-sources/) ...\n",
    "\n",
    "### Narrowing down an interest\n",
    "\n",
    "1. **Choosing my data source:** As a Canadian with general interests, I'm going to start at StatsCan. One thing to keep in mind with them is that they have both public and private datasets. Many of the public ones are just snapshots from a single census, and the resulting table is easy to understand but not very deeply informative. However, they do have some datasets with longitudinal aspects, so I'll look for one of those that's more worthwhile throwing programmatic data science at.\n",
    "\n",
    "2. **Exploring data sets:** There are a lot of subjects on StatsCan: agriculture and food, business, children, construction, crime, digital society, economy, education, energy, families, health, and more. Seeing a list like this is wonderful since these are exactly the sort of prompts students need to lead their curiosity. In my case, I'll click on agriculture and food. It has a lot of datasets and an obvious interest. :)\n",
    "<br><br>\n",
    "This brings me to a set of about 650 items, of which almost 600 are data tables. I'll limit myself to those since those are the easiest way to ingest data such that we can work with it in code.\n",
    "<br><br>\n",
    "That's a lot of tables to wade through, so I'll use the subcategory filter to limit it to food (130 tables). One of the top ones is \"Food available in Canada\" (in kilos per person per year). This sound vaguely interesting. Another one is [\"Supply and disposition of food in Canada\"](https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=3210005301), which sounds to me like how it was used, and thus a bit richer. I'll use that.\n",
    "\n",
    "<img width=\"600\" src=\"assets/screenshots/sc_002.png\" />\n",
    "\n",
    "3. **Preliminary skim:** As we saw in the last notebook, it's possible to preview datasets in Python using `pandas`, but since we want to get a sense of what's out there without having to download all these tables, I'll just skim through it in the browser.\n",
    "<br><br>\n",
    "I click on \"Supply and disposition of food in Canada\". On StatsCan, you can choose various filters for the table views. Here I can choose one of many commodities, and I can choose a reference period. Wonder of wonders, the reference period lets me go back all the way to 1960! This is a jackpot dataset — lots to analyze. I can see that the resulting tables give me supply (broken down into stock, production, imports) vs. disposition (domestic disappearance, exports, manufacturing, and waste). I think this dataset will be worth poking around.\n",
    "\n",
    "<img width=\"600\" src=\"assets/screenshots/sc_001.png\" />\n",
    "\n",
    "### Downloading the data\n",
    "\n",
    "On StatsCan, when you click on `Download Options`, you get quite a range. I'm going to choose the option to download the entire table as a CSV. That gives us the most flexibility to work with it in code.\n",
    "\n",
    "<img width=\"600\" src=\"assets/screenshots/sc_003.png\" />\n",
    "\n",
    "The resulting file, which can be found locally as well [here](data/32100053.csv), is about 10 MB with 16 columns and 70,000 rows for a total of ~1.1 million cells. (This sounds like a lot, but it's far from the largest datasets out there! But it will do fine for our purposes.)\n",
    "\n",
    "### Cleaning the data\n",
    "\n",
    "#### Why clean?\n",
    "\n",
    "Why do we clean data? Because datasets that comes to us from the wild may have various formats (with inconsistent parsing rules), invalid data, missing data, and more. For example, suppose you had data from a number of volunteers who wrote down something like observations of birds with timestamps, but across your volunteers you find timestamps formatted as `3:01`, `3:01 PM`, `3:01 p.m`., `15:03`, `June 1 2023 3:03 P.M. EST`, and so forth. You would need to normalize them to one format so that they could be compared in analysis. Similarly, perhaps you have some numerical data on temperatures, but some of the rows fail to indicate whether the system was in Celsius or Fahrenheit and need to be thrown out.\n",
    "\n",
    "In the case of StatsCan, the dataset already comes to us pretty \"clean\", with well-labelled and predictable formats, but it can still have missing data where there are gaps in their records (usually represented by `-` or `..` in their tables). It will also have a lot of extraneous data, e.g. metadata, columns linking the data with other records, or columns where every row has the same value (e.g. \"Canada\" because we downloaded a composite of all geographic regions).\n",
    "\n",
    "#### How to clean?\n",
    "\n",
    "There are essentially three ways to clean data:\n",
    "\n",
    "* Manually, using a text editor or dedicated CSV editor. For example, VSCode has numerous CSV extensions that let you add or reorder columns and so forth. (Note that while you can use Excel for this, it tends to alter your CSV format in ways you might not expect.)\n",
    "\n",
    "* Via scripting that edits and resaves the CSV file. For example, Python has a built-in CSV module that we could use to read the file row by row, perform operations on rows and columns, etc.\n",
    "\n",
    "* In code at runtime using table-oriented libraries like `pandas`, as we saw in the previous notebook.\n",
    "\n",
    "| Method | Pros | Cons\n",
    "| - | - | -\n",
    "| Manual editing | Requires less technical skill; fast to set up | Limited to find-and-replace or column/row-wise editing\n",
    "| Scripting using `csv` module | Relatively fast and powerful | No model, so somewhat unintuitive to edit data via indices rather than headers\n",
    "| At runtime in `pandas` library | Powerful and intuitive editing | May require pre-cleaning data; slower, is executed again on each run\n",
    "\n",
    "#### Cleaning, part 1\n",
    "\n",
    "Our food dataset is sizable, and a quick glance at the table suggests that it can be trimmed quickly, so I'll do this in three passes: first with manual editing for the broad cleaning, then scripting, and finally with `pandas` at runtime for finer-grained selection from the trimmed dataset.\n",
    "\n",
    "I'll begin by making a copy of our `csv` with a different name, [`food_clean_1.csv`](data/food_clean_1.csv). We always preserve our original data in case something goes wrong, since our cleaning operations can be destructive.\n",
    "\n",
    "Opening it in VSCode's extension `Edit csv`, I see its 16 columns are:\n",
    "\n",
    "`REF_DATE,GEO,DGUID,Commodity,Supply and disposition,UOM,UOM_ID,SCALAR_FACTOR,SCALAR_ID,VECTOR,COORDINATE,VALUE,STATUS,SYMBOL,TERMINATED,DECIMALS`\n",
    "\n",
    "I need `REF_DATE` (the year), `Commodity`, `Supply and disposition`, and `Value`. The rest are empty, metadata, or repeated values. (Even `UOM` (unit of measure) and `SCALAR_FACTOR` (unit multiplier) are just \"Tonnes\" and \"thousands\" in every case, so they are needless on a row-by-row basis.) Incidentally, you might also wonder where the columns went that indicated finer detail like production, imports, domestic disappearance, and exports. It turns out that each commodity occupies up to 10 rows, and those finer details each have their own row. We'll deal with that later. For now, I'll just delete the unnecessary columns. Also, I'll rename `REF_DATE` to `Year`.\n",
    "\n",
    "That's probably about all we can do with manual editing. Some of the things we'd like to be able to do as we skim through the resulting file (which takes up about 1/3 of the space).\n",
    "\n",
    "* Convert the rows into columns (since they consist of 8 identical rows that differ only in the value column)\n",
    "* Identify and possibly remove empty cells\n",
    "* Possibly eliminate data we're not interested in (this requires some thinking about what we want to do with the data)\n",
    "\n",
    "#### Cleaning, part 2\n",
    "\n",
    "Let's start with the conversion of rows into columns. This is something that can be done fairly easily with normal Python logic, but would be somewhat involved if using `pandas` `DataFrames`, so let's use the second method.\n",
    "\n",
    "Our algorithm will be to read each row, identify the first 2 columns, and create a row in a new CSV, and then append successive rows' \"Value\" columns until the first 2 columns change.\n",
    "\n",
    "One minor difficulty we notice as we skim the data is that not all commodities have the full breakdown. This means that we will need to read the label for the available data rather than just relying on an index. (StatsCan has provided somewhat \"dirty\" data, in that true predictability would have been to mark missing data with blank cells, but instead they omitted the rows altogether. That's what makes our job fun...!)\n",
    "\n",
    "Here's some code to clean the CSV. It outputs [`food_clean_2.csv`](data/food_clean_2.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass of food data cleaning, using Python's built-in csv module\n",
    "import csv\n",
    "\n",
    "# Open old and new CSVs. Blank newline because csv adds '\\n' anyway\n",
    "with open('data/food_clean_1.csv', 'r') as f_old:\n",
    "    with open('data/food_clean_2.csv', 'w', newline='') as f_new:\n",
    "        reader = csv.reader(f_old)\n",
    "        writer = csv.writer(f_new)\n",
    "\n",
    "        # Omit the headers; we know them\n",
    "        next(reader)\n",
    "\n",
    "        # Write the headers for the new file\n",
    "        writer.writerow(['Year', 'Commodity', 'Total supply', 'Beginning stocks', 'Production', 'Imports', 'Total disposition', 'Domestic disappearance', 'Exports', 'Waste', 'Ending stocks'])\n",
    "\n",
    "        # Row template\n",
    "        # This is the messy part, where we need to be able to put each value in the right column\n",
    "        # Note: I'm also combining \"Production\" and \"Manufacturing\"\n",
    "        col_indices = {'Total supply': 2, 'Beginning stocks': 3, 'Production': 4, 'Manufacturing': 4, 'Imports': 5, 'Total disposition': 6, 'Domestic disappearance': 7, 'Exports': 8, 'Waste': 9, 'Ending stocks': 10}\n",
    "\n",
    "        # Begin constructing the slimmer CSV's row\n",
    "        new_row = [''] * 11\n",
    "\n",
    "        # Examine each row\n",
    "        for old_row in reader:\n",
    "\n",
    "            # Write a finished row\n",
    "            if new_row[0] and (old_row[:2] != new_row[:2]):\n",
    "                writer.writerow(new_row)\n",
    "                new_row = [''] * 11\n",
    "            \n",
    "            # Construct said row\n",
    "            new_row[0] = old_row[0]\n",
    "            new_row[1] = old_row[1]\n",
    "\n",
    "            label = old_row[2]\n",
    "            col_index = col_indices[label]\n",
    "            value = old_row[3]\n",
    "\n",
    "            new_row[col_index] = value\n",
    "        \n",
    "        # After the loop, write the final row\n",
    "        writer.writerow(new_row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have quite a manageable dataset: under 1 MB with ~12,000 rows and 4 columns. Further filtration can be done at runtime in `pandas`. I suspect that we'll want to look through it a bit rather than attempting to clean it further at this point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring, analysing, and visualizing data\n",
    "\n",
    "### Some potential questions\n",
    "\n",
    "To discover some questions worth analyzing, we should poke our head around the data a little.\n",
    "\n",
    "First of all, the column names themselves suggest some interesting questions. One might ask ones about changes over time:\n",
    "\n",
    "* How has our production vs. importing changed as a function of our total supply? Do we import more or less than we used to?\n",
    "\n",
    "* How has waste changed over time (again as a function of total disposition)? Are we more or less wasteful now?\n",
    "\n",
    "* Has supply scaled with population? Has disposition scaled with population? (These ones require a secondary data source to correlate population.)\n",
    "\n",
    "* Do our stockpiles tend to grow over time or do they remain the same?\n",
    "\n",
    "These questions could be asked based on totals for the years or could be zoomed into a particular commodity we're interested in.\n",
    "\n",
    "We could also ask questions based on commodities:\n",
    "\n",
    "* Which things do we mostly produce and which do we mostly import?\n",
    "\n",
    "* Which things do we mostly consume domestically and which do we mostly export?\n",
    "\n",
    "* Which things are we most wasteful of?\n",
    "\n",
    "* How has the number of commodities we track changed over the years? (For this we could consider number of blank cells, for instance.)\n",
    "\n",
    "### Subcategories\n",
    "\n",
    "One downside I note is that there are 190 different commodities, the categories aren't that obvious. A more careful reading by year, however, suggests that each year contains the categories: flours, misc, beverages, dairy, meat, oils, fruits, vegetables, fish. We probably do want to combine those if we want to do any comparison between foods, since otherwise we'll be comparing 1% skim milk against 3% skim milk and so forth.\n",
    "\n",
    "There are a few ways to do this. My way will be to use more Python scripting to create (1) a list of unique values and (2) a CSV in which I will manually map values to categories. Because this will be quite a small map, I'll consult it at runtime when doing my data analysis rather than creating a third revision of our dataset.\n",
    "\n",
    "Here's some code that creates the basis for subcategory mapping. It generates [food_categories_stub.csv](data/food_categories_stub.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subcategory mapping\n",
    "import csv\n",
    "\n",
    "# I'll use a set to track the unique commodities since sets inherently resolve duplicates\n",
    "unique_commodities = set()\n",
    "\n",
    "with open('data/food_clean_2.csv', 'r') as f_data:\n",
    "        reader = csv.reader(f_data)\n",
    "\n",
    "        # Skip the headers\n",
    "        next(reader)\n",
    "\n",
    "        # Put each commodity into the set\n",
    "        for row in reader:\n",
    "            commodity = row[1]\n",
    "            unique_commodities.add(commodity)\n",
    "\n",
    "# Write them as a CSV in which I can manually add categories\n",
    "with open('data/food_categories_stub.csv', 'w', newline='') as f_map:\n",
    "    writer = csv.writer(f_map)\n",
    "\n",
    "    # Write the headers\n",
    "    writer.writerow(['Commodity', 'Category'])\n",
    "    \n",
    "    # Write each row with a blank space in the category column\n",
    "    for commodity in sorted(unique_commodities):\n",
    "         writer.writerow([commodity, ''])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll now create a copy called [food_categories.csv](data/food_categories.csv) (the reason I create a copy is because the above code could easily overwrite and blank out my manual work otherwise). This is a great task to give a student or committee for half an hour or so — labelling underspecified data is an important part of preparing a dataset. A committee could provide some great discussion since they might disagree. For example, I included \"potato chips\" under vegetables along with potatoes, but someone else might put them under miscellaneous or junk food. And I put canned and frozen vegetables together with fresh, but an alternative study might explore Canadian trends between these specific categories, for example. (One could always produce multiple mappings for different studies of the same data.)\n",
    "\n",
    "Also, while doing this, I noticed that some rows only sum up other rows (e.g. \"Beef and veal total\"), which could lead to incorrect sums for categories if we include them. Thus, I will create a third revision of the dataset afterwards that eliminates these rows. Here's the code for that. It produces [food_clean_3.csv](data/food_clean_3.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that are just totals of other rows\n",
    "import csv\n",
    "\n",
    "with open('data/food_clean_2.csv', 'r') as f_old:\n",
    "    with open('data/food_clean_3.csv', 'w', newline='') as f_new:\n",
    "        reader = csv.reader(f_old)\n",
    "        writer = csv.writer(f_new)\n",
    "\n",
    "        # Write all rows except those that contain the word 'total'\n",
    "        # (Coarse but in skimming the rows for my categories I know it's sufficient)\n",
    "        for row in reader:\n",
    "            if 'total' not in row[1].lower():\n",
    "                writer.writerow(row)\n",
    "           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling in `pandas`\n",
    "\n",
    "Now it's finally time to get into the `pandas` material that we learned last time and make some `DataFrames`. We're going to do some preliminary exploration of trends to try to generate more specific questions.\n",
    "\n",
    "### Making our first `DataFrame`\n",
    "\n",
    "Let's ingest the CSV files we made.\n",
    "\n",
    "P.S. Two reminders:\n",
    "\n",
    "* Don't forget to run `pip install pandas` in your terminal if you haven't yet.\n",
    "\n",
    "* In Jupyter Notebooks, cells share a memory pool, so which cells you run and which order you run them in can affect the result. You can always hit the `Run all` button to catch up on any that were missed so that future cells work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingesting CSVs into a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# Beautifully simple\n",
    "df = pd.read_csv('data/food_clean_3.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insert a column for the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a category column\n",
    "\n",
    "# Ingest the category CSV and turn it into a dict\n",
    "df_com_cats = pd.read_csv('data/food_categories.csv')\n",
    "com_to_cat = {k: v for (i, k, v) in df_com_cats.itertuples()}\n",
    "\n",
    "# Create a column by mapping commodity values into the dict\n",
    "cat_column = df['Commodity'].map(com_to_cat)\n",
    "\n",
    "# Insert said column into the main DataFrame at index 1 with the label 'Category'\n",
    "df.insert(1, 'Category', cat_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we can view the head (or first few rows) of the `DataFrame` like this in order to verify its structure and contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year Category                Commodity  Total supply  Beginning stocks  \\\n",
      "0  1960  Staples              Wheat flour       1934.34             87.07   \n",
      "1  1960  Staples                Rye flour          8.60              0.38   \n",
      "2  1960  Staples  Oatmeal and rolled oats         44.07              1.69   \n",
      "3  1960  Staples     Pot and pearl barley          1.67              0.11   \n",
      "4  1960  Staples      Corn flour and meal         16.41              0.25   \n",
      "\n",
      "   Production  Imports  Total disposition  Domestic disappearance  Exports  \\\n",
      "0       45.07     0.08            1842.27                 1060.01   715.56   \n",
      "1        7.88     0.34               8.36                    8.19      NaN   \n",
      "2       42.38      NaN              42.52                   38.43     3.31   \n",
      "3        1.56      NaN               1.60                    1.57     0.01   \n",
      "4        9.00     7.16              16.13                   13.35     2.51   \n",
      "\n",
      "   Waste  Ending stocks  \n",
      "0  21.63          92.07  \n",
      "1   0.17           0.24  \n",
      "2   0.78           1.55  \n",
      "3   0.02           0.07  \n",
      "4   0.27           0.28  \n"
     ]
    }
   ],
   "source": [
    "# View DataFrame head\n",
    "print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other views of the data\n",
    "\n",
    "Let's create more dataframes as \"views\" of this content. First, a year-wise one with totals of all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Total supply  Beginning stocks  Production  Imports  Total disposition  \\\n",
      "Year                                                                           \n",
      "1960      13853.38           1432.88     4111.26  1753.66           11224.18   \n",
      "1961      14267.65           1577.80     4361.02  1703.68           11672.31   \n",
      "1962      14826.65           1834.87     4486.84  1657.62           12020.56   \n",
      "1963      15344.77           1884.36     4672.45  1696.54           12681.38   \n",
      "1964      16044.22           1939.14     4964.80  1697.89           13315.62   \n",
      "\n",
      "      Domestic disappearance  Exports   Waste  Ending stocks  \n",
      "Year                                                          \n",
      "1960                10662.27   980.44  574.44        1577.80  \n",
      "1961                10757.33  1055.50  587.83        1834.87  \n",
      "1962                11233.38  1091.39  604.25        1879.59  \n",
      "1963                11738.68  1197.91  601.90        1939.14  \n",
      "1964                11899.07  1664.03  613.95        1872.21  \n"
     ]
    }
   ],
   "source": [
    "# Year-wise DataFrame\n",
    "\n",
    "# Set the index to the year\n",
    "df_years = df.set_index('Year', drop=True)\n",
    "\n",
    "# Group values by shared year and sum them (ignoring blank cells)\n",
    "df_years = df_years.groupby(df_years.index).sum(numeric_only=True)\n",
    "\n",
    "print(df_years.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-level frame\n",
    "\n",
    "Nice. Now, suppose we want to create a category-wise `DataFrame`. This suggests that our rows are categories. (We'll eliminate commodities for this frame.) What will the columns be? Suppose we say years. If so, we must pick one of the measures to serve as the vector. For example, if our frame looked like this:\n",
    "\n",
    "| Category | 1960 | 1961 | ... | 2022\n",
    "| - | - | - | - | -\n",
    "| Staples | 200.2 | 250.4 | ... | 678.2\n",
    "| Beverages | 37.3 | 49.1 | ... | 163.0\n",
    "\n",
    "Those numbers can seemingly only represent one of our previous measures: total supply, or total disposition, or perhaps imports or exports or waste.\n",
    "\n",
    "What then? Do we need a separate dataframe for each of the 10 measures? Perhaps we could do that with a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating a category-wise dataframe for a given column\n",
    "\n",
    "def make_catwise(column: str) -> pd.DataFrame:\n",
    "    df_cats = pd.DataFrame()\n",
    "    df_cats['Category'] = df['Category'].unique()\n",
    "    df_cats = df_cats.set_index('Category', drop=True)\n",
    "\n",
    "    # Go through each year\n",
    "    for year in df['Year'].unique():\n",
    "        df_cats[year] = 0.0\n",
    "\n",
    "        # Sum the years for the supplied column for each category\n",
    "        for cat in df_cats.index.get_level_values('Category'):\n",
    "            data = df[(df['Year'] == year) & (df['Category'] == cat)][column]\n",
    "            total = sum(data.fillna(0))\n",
    "            df_cats.loc[cat, year] = total\n",
    "\n",
    "    return df_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  1960     1961     1962     1963     1964     1965     1966  \\\n",
      "Category                                                                       \n",
      "Staples        2042.90  1953.71  1895.29  2074.76  2307.88  2155.07  2126.68   \n",
      "Miscellaneous  1123.06  1144.71  1205.78  1240.62  1220.53  1275.74  1324.17   \n",
      "Vegetables     4823.68  5048.94  5407.63  5273.20  5512.43  5558.48  5968.45   \n",
      "Beverages        87.84    94.28    98.68   104.57   101.41   101.38    96.09   \n",
      "Dairy          1976.75  2160.15  2208.34  2286.16  2356.69  2656.48  2732.45   \n",
      "\n",
      "                  1967     1968     1969  ...     2013     2014     2015  \\\n",
      "Category                                  ...                              \n",
      "Staples        1947.47  1913.38  1929.39  ...  2665.91  2406.06  2445.21   \n",
      "Miscellaneous  1315.93  1320.08  1350.69  ...  1944.32  1955.77  1898.06   \n",
      "Vegetables     6042.67  6205.26  6195.20  ...  5112.01  5104.00  5198.70   \n",
      "Beverages       109.33   121.80   122.49  ...   365.29   405.31   365.01   \n",
      "Dairy          2801.05  2918.36  3015.31  ...  4279.24  4281.86  4345.49   \n",
      "\n",
      "                  2016     2017     2018     2019     2020     2021     2022  \n",
      "Category                                                                      \n",
      "Staples        2450.85  2486.63  2526.09  2569.17  2580.42  2510.72  2621.17  \n",
      "Miscellaneous  1953.80  2078.90  2125.84  2145.73  2243.70  2258.25  2283.22  \n",
      "Vegetables     5315.48  5328.27  5509.46  5553.37  5536.72  5669.57  5675.59  \n",
      "Beverages       386.20   390.89   396.12   414.90   406.81   430.76   463.86  \n",
      "Dairy          4384.02  4354.82  4369.30  4323.97  4389.33  4266.74  4205.51  \n",
      "\n",
      "[5 rows x 63 columns]\n"
     ]
    }
   ],
   "source": [
    "# Testing said function\n",
    "df_cat_supply = make_catwise('Total supply')\n",
    "print(df_cat_supply.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it seems unintuitive to have 10 separate DataFrames that each focus on one measure.\n",
    "\n",
    "Instead, we can use a [`MultiIndex`](https://pandas.pydata.org/docs/user_guide/advanced.html). Here, indices consist of multiple hierarchical levels. If that sounds complex, here's a table illustration:\n",
    "\n",
    "| Category | Year | Total supply | Total disposition\n",
    "| - | - | - | - \n",
    "| Staples | 1960 | 200.2 | 196.4\n",
    "| | 1961 | 250.4 | 212.3\n",
    "| | ... | ... | ...\n",
    "| | 2022 | 678.2 | 654.7\n",
    "| Beverages | 1960 | 37.3 | 33.4\n",
    "| | 1961 | 49.1 | 48.9\n",
    "| | ... | ... | ...\n",
    "| | 2022 | 163.0 | 155.6\n",
    "\n",
    "This hierarchical indexing lets us capture each year *and* measure while still orienting it around just one of the original columns (Category).\n",
    "\n",
    "Here's how we make it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Total supply  Beginning stocks  Production  Imports  \\\n",
      "Category  Year                                                        \n",
      "Beverages 1960         87.84               0.0         0.0    87.84   \n",
      "          1961         94.28               0.0         0.0    94.28   \n",
      "          1962         98.68               0.0         0.0    98.68   \n",
      "          1963        104.57               0.0         0.0   104.57   \n",
      "          1964        101.41               0.0         0.0   101.41   \n",
      "\n",
      "                Total disposition  Domestic disappearance  Exports  Waste  \\\n",
      "Category  Year                                                              \n",
      "Beverages 1960              87.84                 1262.56     0.50    0.0   \n",
      "          1961              94.28                 1301.38     1.86    0.0   \n",
      "          1962              98.68                 1357.12     2.30    0.0   \n",
      "          1963             104.57                 1424.78     4.61    0.0   \n",
      "          1964             101.41                 1468.73     2.82    0.0   \n",
      "\n",
      "                Ending stocks  \n",
      "Category  Year                 \n",
      "Beverages 1960            0.0  \n",
      "          1961            0.0  \n",
      "          1962            0.0  \n",
      "          1963            0.0  \n",
      "          1964            0.0  \n"
     ]
    }
   ],
   "source": [
    "# Create a category-wise DataFrame with a MultiIndex\n",
    "\n",
    "# Set the multi-index to category and year; drop commodity definition\n",
    "df_cats = df.set_index(['Category', 'Year'])\n",
    "df_cats = df_cats.drop(columns='Commodity')\n",
    "\n",
    "# Sum the years\n",
    "df_cats = df_cats.groupby(level=[0, 1]).sum(numeric_only=True)\n",
    "\n",
    "print(df_cats.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the power of `pandas DataFrames`... think of all the iteration and comparison we would have done on regular Python data structures!\n",
    "\n",
    "Note that seeing this logic, it will be easily to reclaim the commodity-level specifity if we want to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Total supply  Beginning stocks  Production  \\\n",
      "Commodity                   Year                                               \n",
      "Ale, beer, stout and porter 1960           0.0               0.0         0.0   \n",
      "                            1961           0.0               0.0         0.0   \n",
      "                            1962           0.0               0.0         0.0   \n",
      "                            1963           0.0               0.0         0.0   \n",
      "                            1964           0.0               0.0         0.0   \n",
      "\n",
      "                                  Imports  Total disposition  \\\n",
      "Commodity                   Year                               \n",
      "Ale, beer, stout and porter 1960      0.0                0.0   \n",
      "                            1961      0.0                0.0   \n",
      "                            1962      0.0                0.0   \n",
      "                            1963      0.0                0.0   \n",
      "                            1964      0.0                0.0   \n",
      "\n",
      "                                  Domestic disappearance  Exports  Waste  \\\n",
      "Commodity                   Year                                           \n",
      "Ale, beer, stout and porter 1960                 1070.44      0.0    0.0   \n",
      "                            1961                 1099.34      0.0    0.0   \n",
      "                            1962                 1145.37      0.0    0.0   \n",
      "                            1963                 1200.16      0.0    0.0   \n",
      "                            1964                 1246.50      0.0    0.0   \n",
      "\n",
      "                                  Ending stocks  \n",
      "Commodity                   Year                 \n",
      "Ale, beer, stout and porter 1960            0.0  \n",
      "                            1961            0.0  \n",
      "                            1962            0.0  \n",
      "                            1963            0.0  \n",
      "                            1964            0.0  \n"
     ]
    }
   ],
   "source": [
    "# Create a commodity-wise DataFrame with a MultiIndex\n",
    "\n",
    "# Set the multi-index to commodity and year\n",
    "df_coms = df.set_index(['Commodity', 'Year'])\n",
    "\n",
    "# Sum the years\n",
    "df_coms = df_coms.groupby(level=[0, 1]).sum(numeric_only=True)\n",
    "\n",
    "print(df_coms.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketching visualization, part 1\n",
    "\n",
    "We'll begin with some light visualization of various aspects of our data just in order to get a sense of the sorts of insights it might yield. That is, we will turn numbers into images so they can be scanned more readily.\n",
    "\n",
    "Our library for this will be [`plotly`](https://plotly.com/python/), and specifically [`plotly.express`](https://plotly.com/python/plotly-express/). You will also need `iPython` if you don't have it installed in order to display the visual results in a notebook.\n",
    "\n",
    "So go ahead and run:\n",
    "\n",
    "```\n",
    "pip install plotly\n",
    "pip install ipython\n",
    "```\n",
    "\n",
    "Note: depending on your IDE, for visual librairies, restarting your kernel may not be sufficient. You may actually have to close and reopen the IDE. That's what I have to do with VSCode on my staff Macbook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we could create a line graph for one dimension of our data: total supply of all products over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexpress\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpx\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mline(df_years, x\u001b[39m=\u001b[39mdf_years\u001b[39m.\u001b[39mindex, y\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal supply\u001b[39m\u001b[39m\"\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal food supply in Canada\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m fig\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/plotly/basedatatypes.py:3390\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3357\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3358\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3359\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3386\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[0;32m-> 3390\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/plotly/io/_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         )\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "# Basic line graph\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df_years, x=df_years.index, y=\"Total supply\", title='Total food supply in Canada')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of features to effort is pretty fantastic.\n",
    "\n",
    "Also, we might suspect that our data has some issues given the unexpected dropoff in 2002. Perhaps the system of tracking changed, and we can only really track either 1960–2001 or 2002–2022.\n",
    "\n",
    "We could actually add more lines if we use the columns as a set of y axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple lines\n",
    "fig = px.line(df_years, x=df_years.index, y=['Total supply', 'Total disposition'],\n",
    "              \n",
    "              # In this case we have to specify the y axis label, since it defaults\n",
    "              # to 'value' when you use more than one series of data for an axis\n",
    "              title='Food supply and disposition in Canada', labels={'value': 'Thousands of tonnes'})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving data quality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this graph is delightful, we now have a more unsettling observation to make: in 1978, total disposition somehow exceeds total supply (despite the fact that the latter includes beginning stock), and it never goes back under. What can be happening?\n",
    "\n",
    "I have a suspicion that missing data might account for this. We could create a `DataFrame` that counts the missing cells per column, with or without an index by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting total NaNs\n",
    "df_nans = df.isna().sum()\n",
    "\n",
    "print(df_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting NaNs by year\n",
    "df_nans_by_year = df.isna().groupby(df.Year).sum()\n",
    "print(df_nans_by_year.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot. Let's produce a cleaner set of `DataFrames` that omit all commodities with missing cells. Go ahead and run all these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year-wise, version 2\n",
    "df_years_2 = df.set_index('Year', drop=True)\n",
    "\n",
    "# This is the new line: we select only rows where it is not (~) the case\n",
    "# that there is a NaN value in any of the columns (axis=1)\n",
    "df_years_2 = df_years_2[~df_years_2.isna().any(axis=1)]\n",
    "\n",
    "df_years_2 = df_years_2.groupby(df_years_2.index).sum(numeric_only=True)\n",
    "\n",
    "print(df_years_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commodity-wise, version 2\n",
    "df_coms_2 = df.set_index(['Commodity', 'Year']).drop('Category', axis=1)\n",
    "df_coms_2 = df_coms_2[~df_coms_2.isna().any(axis=1)]\n",
    "df_coms_2 = df_coms_2.groupby(level=[0, 1]).sum(numeric_only=True)\n",
    "\n",
    "print(df_coms_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-wise, version 2\n",
    "df_cats_2 = df.set_index(['Category', 'Year']).drop('Commodity', axis=1)\n",
    "df_cats_2 = df_cats_2[~df_cats_2.isna().any(axis=1)]\n",
    "df_cats_2 = df_cats_2.groupby(level=[0, 1]).sum(numeric_only=True)\n",
    "\n",
    "print(df_cats_2.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try our line graphs again. The numbers will be much lower, but our data won't have holes in it anymore. Note that we could have easily filtered out this gappy data in other ways. For example, instead of omitting rows that had a missing cell in any column, we could have eliminated columns we were uninterested in first; that might leave us with more data. We might return to this later on. For now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muliple-line graph on cleaner data\n",
    "\n",
    "fig = px.line(df_years_2, x=df_years_2.index, y=['Total supply', 'Total disposition'],\n",
    "              title='Food supply and disposition in Canada', labels={'value': 'Thousands of tonnes'})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Disposition is now always less than or equal to supply, as we would have hoped. We still have that dramatic shift in 2002 suggesting that the data collection or tallying method changed. There are also odd spikes at 1965 and 1988 Let's limit ourselves to 2002–2022 from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year-wise, version 3\n",
    "df_years_3 = df_years_2.loc[df_years_2.index > 2001]\n",
    "print(df_years_3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commodity-wise, version 3\n",
    "\n",
    "# Slightly more complex since we have a MultiIndex for these ones\n",
    "df_coms_3 = df_coms_2.loc[df_coms_2.index.get_level_values('Year') > 2001]\n",
    "print(df_coms_3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-wise, version 3\n",
    "df_cats_3 = df_cats_2.loc[df_cats_2.index.get_level_values('Year') > 2001]\n",
    "print(df_cats_3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muliple-line graph on year-restricted data\n",
    "\n",
    "fig = px.line(df_years_3, x=df_years_3.index, y=['Total supply', 'Total disposition'],\n",
    "              title='Food supply and disposition in Canada', labels={'value': 'Thousands of tonnes'})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketching visualization, part 2\n",
    "\n",
    "Let's explore a few more ways to look at this data before we dive into some of the bigger insights we could draw out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filled (stacked) area plot for one category's supply\n",
    "cat = df_cats_3.loc['Vegetables']\n",
    "fig = px.area(cat, x=cat.index, y=cat.columns[1:4], title='Yearly supply of vegetables in Canada',\n",
    "              labels={'value': 'Tonnes x 1,000'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filled (stacked) area plot for one commodity's disposition\n",
    "com = df_coms_3.loc['Apples fresh']\n",
    "fig = px.area(cat, x=cat.index, y=cat.columns[5:8], title='Yearly disposition of fresh apples in Canada',\n",
    "              labels={'value': 'Thousands of tonnes'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for categories' waste in one year\n",
    "sub = df_cats_3.loc[df_cats_3.index.get_level_values('Year') == 1991]\n",
    "fig = px.bar(sub, x=sub.index.get_level_values('Category'), y='Waste', title='Canadian food waste in 1991',\n",
    "             labels={'Waste': 'Thousands of tonnes of waste', 'x': 'Category'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart of categories' production in one year\n",
    "sub = df_cats_3.loc[df_cats_3.index.get_level_values('Year') == 1991]\n",
    "fig = px.pie(sub, values='Waste', names=sub.index.get_level_values('Category'), title='Canadian food production in 1991')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By poking around the data like this, we can begin to form insights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Richer visualizations\n",
    "\n",
    "Earlier, I posed some potential questions. Let's see if we can suggest answers to some of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **How has our production vs. importing changed as a function of our total supply?\n",
    "\n",
    "Let's try a stacked area plot to graph total production vs. total imports over years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production vs. imports\n",
    "fig = px.area(df_years_3, x=df_years_3.index, y=['Production', 'Imports'],\n",
    "              title='All food production and importing in Canada',\n",
    "              labels={'value': 'Tonnes x 1,000'})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of my question, however, was \"as a function of our total supply\", a.k.a. a part-of-whole chart. While we can see the two areas' relative growth and visually compare them, perhaps we should see them as proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production vs. imports, proportionally\n",
    "prop = df_years_3.drop(['Total supply', 'Beginning stocks', 'Total disposition', 'Domestic disappearance', 'Exports', 'Waste', 'Ending stocks'], axis=1)\n",
    "prop['Denominator'] = prop['Production'] + prop['Imports']\n",
    "prop['Production'] /= prop['Denominator'] / 100\n",
    "prop['Imports'] /= prop['Denominator'] / 100\n",
    "prop = prop.drop('Denominator', axis=1)\n",
    "\n",
    "fig = px.area(prop, x=prop.index, y=prop.columns,\n",
    "              title='All food production and importing in Canada (as proportion)',\n",
    "              labels={'value': '% of supply'})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the share of the supply furnished by imports has steadily grown.\n",
    "\n",
    "Let's bring in a powerful feature of `plotly`: animated charts. Animation means that we can add a slider to quickly compare different subsets, essentially adding a third dimension. Let's do this chart again, but we'll do it as stacked bar chart by category, animated by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_cats_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Production vs. imports, proportionally\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m prop \u001b[39m=\u001b[39m df_cats_3\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mTotal supply\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBeginning stocks\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTotal disposition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDomestic disappearance\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mExports\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWaste\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEnding stocks\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m prop[\u001b[39m'\u001b[39m\u001b[39mDenominator\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m prop[\u001b[39m'\u001b[39m\u001b[39mProduction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m prop[\u001b[39m'\u001b[39m\u001b[39mImports\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m prop[\u001b[39m'\u001b[39m\u001b[39mProduction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m prop[\u001b[39m'\u001b[39m\u001b[39mDenominator\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m100\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_cats_3' is not defined"
     ]
    }
   ],
   "source": [
    "# Production vs. imports, proportionally\n",
    "prop = df_cats_3.drop(['Total supply', 'Beginning stocks', 'Total disposition', 'Domestic disappearance', 'Exports', 'Waste', 'Ending stocks'], axis=1)\n",
    "prop['Denominator'] = prop['Production'] + prop['Imports']\n",
    "prop['Production'] /= prop['Denominator'] / 100\n",
    "prop['Imports'] /= prop['Denominator'] / 100\n",
    "prop = prop.drop('Denominator', axis=1)\n",
    "\n",
    "fig = px.bar(prop, x=prop.index.get_level_values('Category'), y=prop.columns,\n",
    "              animation_frame=prop.index.get_level_values('Year'),\n",
    "              title='All food production and importing in Canada (as proportion)',\n",
    "              labels={'value': '% of supply', 'x': 'Category'})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORE TO DO\n",
    "\n",
    "* How has waste changed over time (again as a function of total disposition)? Are we more or less wasteful now?\n",
    "\n",
    "* Has supply scaled with population? Has disposition scaled with population? (These ones require a secondary data source to correlate population.)\n",
    "\n",
    "These questions could be asked based on totals for the years or could be zoomed into a particular commodity we're interested in.\n",
    "\n",
    "We could also ask questions based on commodities:\n",
    "\n",
    "* Which things do we mostly produce and which do we mostly import?\n",
    "\n",
    "* Which things do we mostly consume domestically and which do we mostly export?\n",
    "\n",
    "* Which things are we most wasteful of?\n",
    "\n",
    "* How has the number of commodities we track changed over the years? (For this we could consider number of blank cells, for instance.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "458dd1d06817a72759ca62d766d5a1c58019d69edba750c2eb07d80bb7630974"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
